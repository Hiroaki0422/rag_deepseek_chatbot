{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAG Pipeline & Service for DeepSeek chatbot to Access\nThis project implements RAG pipeline/service with powerful open sources such as **DeepSeek** and **FAISS Index**. ","metadata":{}},{"cell_type":"markdown","source":"## What is RAG?\nRetrieval-Augmented Generation (RAG) is a technique that enhances large language models (LLMs) by letting them access external information during generation.\n\nInstead of relying solely on what the LLM \"knows,\" RAG retrieves relevant documents from a knowledge base (like a vector database) and uses them as context for a more accurate, up-to-date, and grounded response.\n\nIt combines:\n\n- **Retrieval**: Finding relevant data/documents for a given query.\n\n- **Generation**: Using an LLM to generate an answer using both the query and the retrieved context.\n\n","metadata":{}},{"cell_type":"markdown","source":"## FAISS\nFAISS (Facebook AI Similarity Search) is an open-source library developed by Meta (Facebook) that performs fast similarity search on high-dimensional vectors.\nItâ€™s typically used for:\n- Searching similar documents or images\n- Building Retrieval-Augmented Generation (RAG) systems\n- Finding nearest neighbors in vector space (e.g., embeddings)","metadata":{}},{"cell_type":"markdown","source":"## Setup Environment ","metadata":{}},{"cell_type":"code","source":"# Install required libraries\n!pip install torch transformers faiss-cpu llama-index llama-index-embeddings-huggingface sentence-transformers\n!pip install transformers accelerate bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model & Tokenizer Setup","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Load DeepSeek model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T20:59:45.847263Z","iopub.execute_input":"2025-04-03T20:59:45.847624Z","iopub.status.idle":"2025-04-03T21:07:04.810874Z","shell.execute_reply.started":"2025-04-03T20:59:45.847594Z","shell.execute_reply":"2025-04-03T21:07:04.810169Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1675334659742a4a655fcca28e718d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2cd15d88b0040f59a59bcd6b2e4747d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed62b38509f485085ced6513557e29b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ffe3073d2a44462ba6039c9111c6ea7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed0cf1e4d5e0417c8a5023d81d8bf137"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"467a61ceb9654f45aa98c0b19c173569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"692d29eac73e476a971f15295c7ffa3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef40c730f534c3685352da015094248"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3deeff88aeb04b6b8451f5f2e2cba5ac"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Prompt Wrapper","metadata":{}},{"cell_type":"code","source":"def generate_response(prompt):\n    system = \"You are a helpful assistant.\"\n    messages = [\n        {\"role\": \"system\", \"content\": system},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n\n    # KEEP this line: it returns tensor directly (your model supports this)\n    input_ids = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    # Create an attention mask manually since tokenizer didn't return it\n    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n\n    # Generate output\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        pad_token_id=tokenizer.eos_token_id,\n        max_new_tokens=512\n    )\n\n    return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:07:04.812052Z","iopub.execute_input":"2025-04-03T21:07:04.812261Z","iopub.status.idle":"2025-04-03T21:07:04.816842Z","shell.execute_reply.started":"2025-04-03T21:07:04.812242Z","shell.execute_reply":"2025-04-03T21:07:04.816007Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Embedding & Index Setup","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\nimport faiss\n\n# Load embedding model\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Dummy corpus (for now)\ndocuments = [\n    \"Hiro Oshima is Data Engineer\",\n    \"Hiro loves cats\",\n    \"Hiro loves data science\"\n]\n\n# Embed and build index\ndoc_embeddings = embedding_model.encode(documents)\nindex = faiss.IndexFlatL2(doc_embeddings.shape[1])\nindex.add(np.array(doc_embeddings).astype(\"float32\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:07:04.818383Z","iopub.execute_input":"2025-04-03T21:07:04.818582Z","iopub.status.idle":"2025-04-03T21:07:17.497890Z","shell.execute_reply.started":"2025-04-03T21:07:04.818564Z","shell.execute_reply":"2025-04-03T21:07:17.497213Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f2b6fbd5814442c95b1591d6ea0e2e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4823cd8c81bd446a8d199f26995d8643"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c8ba493e40444ad8260fa2ff6a58d5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b379639928f43948fde3aad803e9ec7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27932a82a65746ecbf296ebe05e737bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97665a151974e8fa3f797fb9948adf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5c5b893ca743009a5ad97ec25eaa69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eba94f686b0c440b8108b1864fd18653"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f7fb5b09c2f4a4c94bbd56b3a9a2062"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b50ff8b8b6a445668d3fb6aa9bc7f720"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81499afbcead4ae487d58370bc103937"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d5fffb93ece4d1d8961e9ad61a03f6d"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## RAG Retrieval Function","metadata":{}},{"cell_type":"code","source":"def get_embedding(text):\n    return embedding_model.encode([text])[0]\n\ndef retrieve_top_k(query, k=3):\n    query_vector = np.array([get_embedding(query)], dtype=np.float32)\n    distances, indices = index.search(query_vector, k)\n    return [documents[i] for i in indices[0]]\n\ndef rag_chatbot(query):\n    # Retrieve top-k documents\n    retrieved_docs = retrieve_top_k(query, k=3)\n    context = \"\\n\".join(retrieved_docs)\n    \n    prompt = f\"Context:\\n{context}\\n\\nUser Question:\\n{query}\"\n    return generate_response(prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:07:17.498787Z","iopub.execute_input":"2025-04-03T21:07:17.498987Z","iopub.status.idle":"2025-04-03T21:07:17.503946Z","shell.execute_reply.started":"2025-04-03T21:07:17.498968Z","shell.execute_reply":"2025-04-03T21:07:17.503094Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"response = rag_chatbot(\"Tell me what you know about Hiro\")\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T21:07:17.504759Z","iopub.execute_input":"2025-04-03T21:07:17.505024Z","iopub.status.idle":"2025-04-03T21:07:24.593183Z","shell.execute_reply.started":"2025-04-03T21:07:17.505003Z","shell.execute_reply":"2025-04-03T21:07:24.592287Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beb0ce2dba1a4d7299775032be5df9b1"}},"metadata":{}},{"name":"stdout","text":"As an AI, I don't have personal experiences or emotions, but I can provide information based on the context you've provided. \n\nHiro Oshima is a Data Engineer. He is passionate about data science and loves to work with data to extract meaningful insights. He is also a cat lover, which aligns with your statement. He is known for his strong work ethic, attention to detail, and ability to work in a team environment.\n\nHowever, I'm an AI and don't have personal experiences or emotions. I don't have a personal knowledge or opinions. I can provide information based on the context you've provided.\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Add More Index","metadata":{}},{"cell_type":"code","source":"new_docs = [\n    \"INSERT NEW DOCUMENT HERE\",\n    \"Hiro wants likes ML engineering \"\n]\nnew_embeddings = embedding_model.encode(new_docs)\nindex.add(np.array(new_embeddings).astype(\"float32\"))\ndocuments.extend(new_docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T04:50:30.354262Z","iopub.execute_input":"2025-04-03T04:50:30.354586Z","iopub.status.idle":"2025-04-03T04:50:30.387061Z","shell.execute_reply.started":"2025-04-03T04:50:30.354562Z","shell.execute_reply":"2025-04-03T04:50:30.386049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}